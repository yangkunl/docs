# 文章(按时间顺序)

**compound memory networks for few-shot video classification （ECCV2018）**(CMN)

这篇文章是最早在视频动作识别领域探索小样本学习的文章，主要提出了一个compound memory network(复合记忆网络)，并且用**meta-learning**的方式进行网络的学习。

![image-20231112175411481](img\image-20231112175411481.png)

左半部分是表示了multi-saliency embedding representation algorithm, 主要作用是接受变长视频的输入，然后得到一个大小固定的video descriptor以表征视频特性。

右半部分的network是以key-value memory network为原型进行设计的，其中key可以理解为视频特征，value为对应的action label，这里的key又是由{abstract memory, constituent key memory}组成的two-layer结构。

每个constituent key中存储的是一个multi-saliency descriptor（视频representation），abstract key中存储的是对stacked constituent keys进行压缩之后的特征值，这里是为了快速检索。

**网络reading以及inference：**给定一个query，直接在abstract memory 中检索最接近的key，这个key对应的value就是predicted action label。

**网络writing：**整个过程类似于数据库的更新，如果value set中没有当前数据类别，那就执行insert操作。如果已经存在当前数据类别，那就用已存在的key信息和新的数据key信息进行memory的更新，更新顺序为先更新底层的constituent memory，再根据更新后的constituent memory更新对应的abstract memory。

**网络training：** 按照inference的方式得到**predicted label**之后，根据GT label进行loss function的设计。

 **TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition （BMVC2019）**

这篇文章主要是受到text sequence matching任务的启发，将few-shot recognition任务也当成匹配任务进行处理，将一个视频看成是segment-level的序列数据。

文章方法主要包含两个部分：

- embedding module；
- relation module；

![image-20231112185522169](img\image-20231112185522169.png)

**embedding stage：**主要目的是将输入视频变成深度特征。网络结构为C3D+Bidirectional GRU，一个视频分成N个segments的情况下，每个segments都通过C3D得到对应的视频特征，然后再将同一个视频的segments特征输入到GRU中以便学习前后文的时序信息。

**relation module：**首先是segment-by-segment attention，因为每个视频的长度可能是不一样的，那么对应的segment数量也是不同的（主要看sample set segments一栏的横条数目），因此需要进行query video和sample video的alignment工作，这里主要是通过引入一个待学习的网络参数来实现的，即利用矩阵运算进行维度转换，得到新的一个aligned version。然后就是comparsion layer，这里最简单的方法包括multiplication，substraction，文章中采用的是用深度网络（Unidiectional GRU + FC）的方式得到relation score，最后对一个episode中的score值用softmax函数得到对应的probability。

这篇文章比较好的地方还在于将方法扩充到了Zero Shot Learning上，对semantic attribute/word vector用了类似的方法进行处理。

**Few-Shot Video Classification via Temporal Alignment （CVPR2020）**(OTAM)

跟TRAN那篇文章一样整体上是基于sequence matching的想法进行query video与support videos的匹配，进而进行小样本视频的识别工作。

文章的方法示意图如下：

![image-20231112190206222](img\image-20231112190206222.png)

**embedding module**：首先对于每个video，先参考TSN中的sparse采样方法得到$T$段，每段一个小的snipet，对于每个snipet都用embed network得到一个维度为$D_f$深度特征。那对于包含T段的单个视频而言，最终得到的视频特征就是$T\times D_f $（参照query video以及query feature map）。

**order temporal alignment：**1）首先是**temporal alignment**的这个概念，给定一个query video，一个support video各自的feature map的情况下，要得到他们之间的匹配程度，首先提出了一个**snipet-level**的匹配想法，参考图中“distance matrices with temporal alignment”的第一个面（后面的面表示多个**support videos**）。一个面（TxT）中每个pixel是一个二值（0或1），当其中一个pixel（l，m）的**值为1的时候表示query video中的第l个snipet和support video中的第m个snipet是匹配上的**，即特征距离接近的。而图中的折线就是连接了所有值为1的pixel而成的。2）构建好temporal alignment的想法之后，剩下的就是怎么样找到最优的这条折线图使得匹配的两个videos之间的distance function最小。优化算法用的是dynamic temporal warping（DTW），即累计距离值：（其中D表示当前snipet对的距离值）(时序对齐)

$\begin{aligned} \gamma(l, m) & =D(l, m) \\ & +\min \{\gamma(l-1, m-1), \gamma(l-1, m), \gamma(l, m-1)\}\end{aligned}$

当然文章在原始DTW上针对具体问题做了改进，通过增加在t=0的前面和t=T的后面取0的方式使得算法突破边界限制。

整个网络的训练其实就是根据距离函数来设计loss function进行的，这边的DTW本身是没有直接需要训练的参数的，在求D(l,m)的时候需要用到**embedding module**提取到的特征，因此网络实际需要训练的参数也就是这个embedding module。

**Temporal-Relational CrossTransformers for Few-Shot Action Recognition(TRX)**

在该文章之前，小样本学习方法都是将query视频和support集视频中最好的 视频相比，或是和所有视频匹配，最后取平均。该方法带来的问题是，同一个动作不同的视频有着不同的长度和速度，因此此种匹配方式不太符合视频的特点。因此作者提出了TRX来解决该问题，同时作者还探索了视频的时序关系。
![image-20231112195659751](C:\Users\19475\AppData\Roaming\Typora\typora-user-images\image-20231112195659751.png)

首先对视频进行稀疏采样，每个视频采样8帧，使用resnet50提取帧特征，得到一个8×2048的特征。因为动作在不同时刻有着不同的外观、很难用单帧表示，因此需要用至少两帧表示（论文中作者通过实验验证了选取2帧和3帧效果最好）。

接来下先介绍取2帧的操作。公式（1）中Φ是一个卷积操作，将C×W×3的特征转换为D维特征（此处D=2048)。选取两帧特征，进行拼接得到动作Q p {Q_p}Q 


### 结论

1、Trx采用两帧或3帧特征进行拼接表示动作特征；将简单的拼接操作使用类似卷积等网络提取出两帧之间的运动信息；
2、损失函数简单地采用的是query到prototype的距离乘以-1