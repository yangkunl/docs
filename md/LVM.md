**Sequential Modeling Enables Scalable Learning for Large Vision Models**

## 创新点

- 纯视觉大模型,不使用任何的文本信息(linguistic data)

  可以表示原始图像和视频以及注释数据源，如语义分割和深度重建，而不需要像素以外的任何元知识(meta-knowledge)。

- 将很大的视觉数据表示为序列,可以进行训练

- 通过设计visual prompts 可以解决不同的视觉任务

## 问题提出

- 大语言模型(LLM)风靡全球,那怎么建立一个大视觉模型(LVM)。在动物世界中,视觉能力并不依靠语言(language),特别是，许多实验表明，非人灵长类动物的视觉世界与人类的视觉世界极为相似。
- 试图效仿LLM从 1.扩充到海量数据 2.通过prompting可以灵活的完成不同的任务(zero-shot)能力
- 解决方式:
  - Data:利用的视觉数据(visual data)应该有着remarkable diversity(多样性),使用 unannotated images and video(未标注数据)。其次,探索了过去十多年的标注数据,使用了一种名为"visual sentence"来表示这些不同的标注,从而不需要除了像素数据之外的任何meta-knowledge,训练集的总规模是16.4亿张图片/帧。
  - Architecture:使用了一个 large transformer architecture(3 billion parameters),使用一个learned tokenzier 将一张图片映射为256个 vector-quantized tokens。
  - Loss function:masked token 使用 自回归预测(自监督),一旦图像/视频/注释图像都可以表示为序列，我们就可以训练模型，使预测下一个标记的交熵损失最小。
- 注意事项:
  - 随着模型大小和数据大小的增加，会出现适当的缩放行为。
  - 通过在测试时设计合适的提示，现在可以 "解决 "许多不同的视觉任务。虽然测试结果并没有显示出定制的、经过专门训练的模型那样高的性能，但一个视觉模型就能解决这么多任务的事实还是相当令人鼓舞的。
  - 发现无监督数据量对各种标准视觉任务的性能有明显的好处。
  - 知道了LVM处理分布外数据和执行新任务的一般视觉推理能力的蛛丝马迹。**但这还需要进一步的研究**。

### 相关概念:

**1.Multi-task Learining(多任务学习)**

>从传统的每个任务一个模型的设置，计算机视觉正慢慢向一个模型执行多个不同任务的方向发展。现有各种多任务学习方法,但它们通常仅限于固定的、预定义的任务数量。最近，受 LLM 中上下文学习启发的方法放弃了任何任务概念，而是让模型直接从输入提示中推断任务。例如，Visual Prompting在测试时接收任务输入/输出示例对和查询图像，将它们串联成一张 2 x 2 的图像，并使用inpainting法生成所需的输出。但是，由于inpainting制是使用 MAE 的一种变体，因此这些方法也继承了同样的缩放问题。

**2.In-context Learning(上下文学习)**

>In Context Learning（ICL）的关键思想是从类比中学习。上图给出了一个描述语言模型如何使用 ICL 进行决策的例子。首先，ICL 需要一些示例来形成一个演示上下文。这些示例通常是用自然语言模板编写的。然后 ICL 将查询的问题（即你需要预测标签的 input）和一个上下文演示（一些相关的 cases）连接在一起，形成带有提示的输入，并将其输入到语言模型中进行预测。值得注意的是，与需要使用反向梯度更新模型参数的训练阶段的监督学习不同，**ICL 不需要参数更新，并直接对预先训练好的语言模型进行预测（这是与 prompt，传统 demonstration learning 不同的地方，ICL 不需要在下游 P-tuning 或 Fine-tuning）**。我们希望该模型学习隐藏在演示中的模式，并据此做出正确的预测。

**3.Auto-regressive Visual Models(自回归视觉模型)**

>使用自回归模型合成视觉数据的想法至少可以追溯到 70 年前。受香农使用 N -grams 合成语言的启发，从 1954 年 Attneave 的开创性论文[5]开始，许多研究都将这一想法应用于像素、图像补丁、视频帧和运动捕捉数据的顺序合成。随着深度模型的流行，新的研究用 RNN 或 CNN 取代 N -grams 进行像素合成。最近，人们提出了基于Transformer的自动回溯视觉生成方法，这些方法与语言相结合，取得了令人印象深刻的图像合成效果。

## 方法

### Data构建

在计算机视觉领域，并没有一个规模和多样性都相当的数据源。因此作者构建了一个规模相当的数据集,**Unified Vision Dataset v1(UVDv1)**,作者利用了许多不同的数据源:

- unlabelled images(未标注的图像)
- images with visual annotation(使用视觉语言标注的图像)
- unlabelled video(未标注的视频)
- video with visual annotation(使用视觉标注的视频)
- 3D synthetic objects

未标注的数据超过80%,其能提供我们需要的多样性(diversity),带有标注的图像分布更为有限，但质量通常更高。视频数据的局限性更大（通常是以人为中心的活动），但其是宝贵的时间数据来源。三维合成物体渲染的多样性最低，但可以为三维结构的行为提供有价值的提示。重要的是，UVDv1 是一个纯视觉数据集，不包含非视觉元数据（如文本)。UVDv1 共包含 16.4 亿张图片。与大型语言模型的另一个重要区别是，语言数据的所有数据都有一个自然、统一的一维结构--文本流。遗憾的是，视觉数据并非如此，不同的数据源都有不同的结构。在这项工作中，其提出将视觉句子作为视觉数据的统一单位，这样我们就能从不同的数据源中训练出可扩展的模型。视觉句子是一个简单的序列，包含一张或多张图片，后面跟一个句末标记（EOS）。

![image-20231205140758971](attachments\image-20231205140758971.png)

- **Single images**

  单张图像本身就代表了视觉句子的最简单形式--$\{ image，EOS\}$。其使用的是 LAION 5B数据集中 14.9 亿张图像的过滤子集。这是数据中最大的一部分，占 88.5%。

- **Image sequences**

  图像序列是视觉句子的一种自然形式。通过以三个不同的步长（10、20 和 30）对视频进行随机采样，形成包含 16 个帧的视觉句子。此外，还利用 Objaverse 数据集中的**合成三维物体**，为各种物体生成以物体为中心的多视角序列。对于每个物体，我们在 1.5 至 2.2 之间对物体中心与摄像机之间的一个半径长度进行采样，并在-45 度至 45 度之间对一个恒定仰角进行采样，然后以 15 度的步长改变方位角，遍历物体的不同视角，并渲染 24 个视角。总共提取了 42000 个这样的序列用于训练，8000 个用于测试。最后，我们还可以将属于同一语义类别的图像表示为一个序列。

- **Images with annotations**

  为了以统一的方式处理不同类型的图像注释，选择将所有**注释表示为图像**。有些数据类型，如语义分割图、边缘图、深度图 和正常图像，已经可以用这种方式表示。对于其他数据类型，我们会针对每种特定的注释类型采用量身定制的方法：

  - object detection:在每个物体周围覆盖一个彩色编码的边界框来创建标注
  - Human Pose:人体骨骼在像素空间中渲染，遵循 OpenPose 格式，使用 MMPose
  - Depth Estimation, Surface Normal, and Edge Detection: 给定 ImageNet 和 COCO 图像后生成注释。
  - Style Transfer, De-rain, De-noise , Low Light Enhancement [89], and Stereo Datasets :使用图像对的形式
  - Colorization:将 ImageNet 图像转换为灰度图像，生成图像对。
  - Inpainting:这个过程包括随机添加黑色方框

  对于同一图像包含 k 个不同注释的数据集，采用了不同的方法：对于每组$ 1 + k $个图像（输入加 $k $个注释），我们随机选择$ m $个元素，其中$ m ≤ n + 1 ≤ 16$。然后将这些 **$m$ 元组

- **Image sequences with annotations**

  对于视觉句子，其采用了两种互补策略。第一种方法类似于我们处理带有成对注释的图像数据的方法：每个视觉句子都是通过连接帧和它们的注释--${frame1,annot1,frame2,annot2,...}$来构建的。第二种方法是将多个帧连同相应的注释进行分组--${frame1,frame2,annot1,annot2,...}$。

### 模型及其他

采用了两阶段的方法：1）训练一个大型视觉标记器(**visual tokenizer**)（对单个图像进行操作),将每个图像转换为visual tokens；2）对每个表示为标记序列的视觉句子训练一个自回归Transformer模型。

![image-20231205143428187](attachments\image-20231205143428187.png)

- **Image Tokenization**

  为了将Transformer模型应用到图像中，先前的研究通常会采用以下方法之一：或者按照scan-line oder将图像划分为patches，并将其作为序列处理；或者使用预先训练好的image tokenizer，如 VQVAE 或 VQGAN ，将图像特征聚类为离散tokens，并同样按照扫描线顺序将其转化为序列。

  本文采用的是后一种方法，因为模型的离散分类输出自然会形成一个概率分布，其可以很容易地从中采样，从而在视觉句子中灵活地根据条件生成新的图像。值得注意的是，tokenizer是在单个图像上独立运行的，而不是同时在整个视觉句子上运行。这种独立性使能够将tokenizer训练与下游Transformer模型分离开来，这样就可以在单张图像的数据集上训练tokenizer，而无需考虑visual sentence的分布情况。

- **Sequence Modeling of Visual Sentences**

  使用 VQGAN 将图像转换为离散标记后，将多幅图像中的离散标记串联成一个一维序列，**从而将视觉句子视为一个统一的序列**。重要的是，对所有视觉句子都一视同仁--不使用任何特殊标记来表示特定的任务或格式。使用交叉熵损失（cross-entropy loss）来训练一个causal Transformer model，其目标是预测下一个标记，这与语言模型的标准方法类似。在所有视觉句子上以相同的方式训练模型，可使模型从上下文而不是特定任务或格式的标记中推断图像之间的关系。这样，模型就有机会推广到其他未见过的视觉句子结构。

- **Inference by Visual Prompting**

  可以很容易地从这一分布中采样，生成新的visual tokens来完成一个视觉句子。要将模型用于下游任务，可以在测试时构建一个部分视觉句子来定义任务，然后应用模型来生成输出。这类似于语言模型中的上下文学习（in-context learning）或计算机视觉中的视觉提示（visual prompting)。

- **Sequential Prompting**

  我们首先采用最直观、最简单的方法来对 LVM 进行视觉提示：顺序推理。这里的提示结构非常简单：向模型展示 7 幅图像的序列，要求它预测下一幅图像（256 个tokens）。

  视频帧预测:

  顺序提示最直接的任务是视频预测。图 6 展示了几种由来自 Kinetics-700 验证集的序列提示。在顶部，7 个画面提示（蓝色边框）之后是预测画面（红色边框)。

  ![image-20231205151026914](attachments\image-20231205151026914.png)

  **Rotation and Category prediction**

  如何用一个合成物体绕任意轴的三维旋转序列来提示模型，使其能够预测进一步的旋转。或者，我们可以将给定类别的项目列表视为一个序列，并预测同一类别中的其他想法，如下图所示。请注意，虽然该系统是在同一 ImageNet 类别的图像组上进行训练的，但这里的提示包括草图，而这些草图在任何注释数据中都没有出现过。

  ![image-20231205151319149](attachments\image-20231205151319149.png)

- **Analogy Prompting**

  一种更复杂的提示结构（我们称之为 "类比提示"）取得了进展。这种方法要求模型理解任意长度和复杂程度的类比，从而测试其高级解释能力。

  **Qualitative Results:**

  根据每个提示，下一张图片就是预测结果。图的上半部分显示了几个定义任务的提示示例，这些任务是训练集的一部分（但这些实际图像在训练时从未见过）。图的下半部分展示了对训练中从未出现过的任务的泛化。更多定性示例请参见附录。

  ![image-20231205151441623](attachments\image-20231205151441623.png)

  **Unseen Tasks and Dataset:**

  介绍了在 Pascal 3D+ 上进行关键点检测的结果，评估采用了阈值为 0.1 的标准关键点正确率（PCK）指标。值得注意的是，LVM 在该数据集上无需训练就能获得 81.2 的 PCK，显示出令人印象深刻的泛化能力。相比之下，我们展示了一些现有的特定任务模型：StackedHourglass的 PCK 为 68.0，MSS-Net的 PCK 为 68.9，StarMap 的 PCK 为 78.6。

- **Miscellaneous Prompts**

  在这里，我们试图通过向模型提供它以前从未见过的各种提示，看看我们能在多大程度上推动我们的模型。图 10 显示了几个效果还不错的提示。图 11 显示了一些不容易用语言描述的提示--在这些任务中，LVM 最终可能会胜过 LLM。在图 13 中，展示了一个典型的视觉推理问题的初步定性结果，该问题是在IQ测试。

  ![image-20231205151704839](attachments\image-20231205151704839.png)

