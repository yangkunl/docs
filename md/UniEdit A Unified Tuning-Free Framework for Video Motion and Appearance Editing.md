#UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing

# overview

为此，我们的工作旨在探索一种无需调整的框架，能够熟练地解决编辑视频运动和外观的复杂性。为了实现这一目标，我们确定了三个技术挑战：1）将文本引导的运动合并到源内容中并非易事，因为直接应用视频外观编辑 [39, 14] 或图像编辑 [5] 方案会导致不良结果结果（如图5所示）； 2）源视频的非编辑内容保存； 3）外观编辑时继承源视频的空间结构。

我们的解决方案 UniEdit 在反转然后生成框架 [35] 中利用预训练的文本到视频生成器（例如 LaVie [56]）的功能，专门用于克服已确定的挑战。特别是，我们引入了三个关键创新：1）为了将文本引导的运动注入源内容中，我们强调了生成器的时间自注意力层对帧间依赖性进行编码的见解。通过这种方式，我们引入了一个辅助运动参考分支来生成文本引导的运动特征，然后通过时间自注意力层将其注入到主编辑路径中。 2）为了保留源视频的未编辑内容，在图像编辑技术[5]的推动下，我们遵循生成器的空间自注意力层对帧内依赖性进行编码的见解。因此，我们引入了辅助重建分支，并将从重建分支的空间自注意力层获得的特征注入到主编辑路径中。 3）为了在外观编辑过程中保留空间结构，我们将主编辑路径的空间注意图替换为重建分支中的空间注意图。这些创新共同使 UniEdit 重新定义了视频编辑的格局。

据我们所知，UniEdit 代表了文本引导、免调整视频动作编辑领域的一次开创性飞跃。此外，其统一架构不仅有利于各种视频外观编辑任务（如图 1 所示），而且还支持图像到视频生成器进行零样本文本图像到视频生成。通过全面的实验，我们展示了 UniEdit 相对于现有最先进方法的卓越性能，突显了其显着推进视频编辑领域发展的潜力。

## Method Overview

![image-20240330175703509](C:\Users\19475\AppData\Roaming\Typora\typora-user-images\image-20240330175703509.png)

使用DDIM inversion后的latent作为初始噪声$z_T$，然后以目标提示$P_t$为条件的预训练UNet从$z_T$开始执行去噪过程，对于motion editing

实现源内容保存和运动控制，我们建议合并一个辅助重建分支和一个辅助运动参考分支来提供所需的源和运动特征，这些特征被注入到主编辑路径中以实现内容保存和运动编辑，为了进一步缓解背景不一致，我们引入了掩模引导的协调方案

### Tuning-Free Video Motion Editing

**Content Preservation on SA-S Modules**:编辑任务的关键挑战是继承原始内容（视频的纹理和背景），为此，我们引入了辅助重建分支。重建路径从与主编辑路径类似的相同反向潜在$z_T$开始，然后使用预训练的 UNet 以源提示 Ps 为条件进行去噪处理，以重建原始帧。

我们将重建路径的注意力特征注入到空间自注意力（SA-S）层的主编辑路径中，以保存内容。在去噪步骤t，主编辑路径中第l个SA-S模块的注意力操作公式为：
$$
\mathrm{SA-S^\mathcal{l}_edit} := \left\{
\begin{array}{ll}
\mathrm{attn}(Q,K,V^r),\quad t>t_0\ \mathrm{and} \ l>l_0 \\
\mathrm{attn}(Q,K,V), \quad \mathrm{otherwise}
\end{array}
\right.
$$
其中$Q,K,V$是是主编辑路径中的特征，$V_r$ 指重建分支中对应SA-S层的值特征，$t_0$和$l_0$是超参数，通过替换空间特征的值，主编辑路径合成的视频保留了源视频的未编辑特征（例如身份和背景），如图7所示。与之前的算法不同的是，其引入了跨帧注意力机制（即使用第一帧/最后一帧的键和值）。

**Motion Injection on SA-T Modules**：在实现上述的内容保留之后，实现上面介绍的内容保留技术后，我们可以获得与源视频内容相同的编辑视频。然而，我们发现输出视频无法正确遵循文本提示 Pt。一个简单的解决方案是增加 t0 和 l0 的值，以便在注入信息的影响和条件文本提示之间取得平衡。然而，这可能会导致内容与原始视频的纹理和结构不同。为了在不牺牲内容一致性的情况下获得所需的运动，我们建议用参考运动来指导主要编辑路径。具体来说，在去噪过程中涉及辅助运动参考分支（也从反转的潜在 zT 开始）。与重建分支不同，运动参考分支以目标提示 Pt 为条件，其中包含所需运动的描述。为了将运动转移到主编辑路径中，我们的核心见解是时间层对合成视频剪辑的帧间依赖性进行建模（如图 6 所示）。受上述观察的启发，我们在主编辑路径的时间自注意力层上设计了注意力图注入：
$$
\mathrm{SA-T^\mathcal{l}_edit} := \left\{
\begin{array}{ll}
\mathrm{attn}(Q^m,K^m,V),\quad t>t_1\ \mathrm{and} \ l>l_1 \\
\mathrm{attn}(Q,K,V), \quad \mathrm{otherwise}
\end{array}
\right.
$$
其中Qm和Km指的是motionreference分支的query和key，在实践中我们简单地将t1和l1设置为零。据观察，时间注意力图的注入可以有效地促进主编辑路径生成与目标提示对齐的运动。为了更好地将运动与源视频中的内容融合，我们还在早期步骤中的主编辑路径和运动参考分支上实现了空间结构控制（参见第 4.2 节）。

### Tuning-Free Video Appearance Editing

执行外观编辑，